# Classifier-Evaluation
Model selection requires systematic reviews because classifier performance varies greatly depending on the dataset and underlying assumptions. A variety of supervised learning algorithms have been thoroughly compared in earlier research, including that of Caruana and Niculescu-Mizil, who identified important trade-offs between computing cost, interpretability, and accuracy. However, there are still unanswered questions about how well these models work with different data partitions and constrained features in contemporary preprocessing procedures. This study aims to close this gap by assessing three popular classifiers, Support Vector Machines (SVM), Logistic Regression (LOGREG), and Random Forests (RF),  on three datasets from the UCI Machine Learning Repository: Spambase, Credit Approval, and Tic-Tac-Toe. We provide a comparison of accuracy trends under various training/testing splits by methodically adjusting hyperparameters and using cross-validation. My results demonstrate that Random Forests consistently achieve state-of-the-art performance, particularly with increased training data, while SVM exhibits competitive but parameter-sensitive behavior. Logistic Regression, despite its simplicity, delivers robust baseline accuracy. These findings reaffirm the importance of tailored model selection and rigorous evaluation to optimize performance in diverse machine learning tasks.
